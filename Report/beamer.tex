\documentclass[8pt]{beamer}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[titlenumbered,ruled,noend,french,onelanguage,linesnumbered]{algorithm2e}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{array}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usetheme{Antibes}
\usepackage{tabulary}



\setbeamertemplate{footline}[frame number]

\title{Débruitage d'image par l'utilisation d'un perceptron multi-couches}
\author{VERNAY Amélie \\ FATTOUHY Mohamed \\ ESTEVE Nathan \\ NGUYEN Louis}

\titlegraphic{\includegraphics[width=2.5cm,height=2.5cm]{../datasets/images/Logo_FDS.png}}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[currentsection]
  \end{frame}
}


\setbeamertemplate{navigation symbols}{}


\begin{document}


\begin{frame}
\titlepage
\end{frame}


\begin{frame}
\frametitle{Sommaire}
\tableofcontents
\end{frame}


\section{Context}

\begin{frame}{Context}
\begin{block}{Aim}
Problematic: Mapping a noisy image (image pixels undergo random fluctuations) to a noisy-free image.
\end{block}\

\begin{block}{Existing algorithm}
%Mettre les 5 autres algo
\end{block}

\begin{block}{}
Suggested in the article : Using patches with a MLP-based method.
%mettre le nom des auteurs
\end{block}


\end{frame}

\section{Noise}

\begin{frame}

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.45]{../datasets/images/Allnoise.png}
        \caption{Representation of Augustin Louis Cauchy with different noise}
    \end{center}
\end{figure}

\end{frame}

\section{MLP-based method}



\begin{frame}{MLP-based method}
% Dessin de MLP + expliqué notion de patches (phase d'entrainement)

% ----
% Input layer neurons'number
\newcommand{\inputnum}{3} 
 
% Hidden layer neurons'number
\newcommand{\hiddennum}{5}  
 
% Output layer neurons'number
\newcommand{\outputnum}{2} 

% Input layer neurons'number
\renewcommand{\inputnum}{1} 
 
% Hidden layer neurons'number
\renewcommand{\hiddennum}{5}  
 
% Output layer neurons'number
\renewcommand{\outputnum}{1} 
% ----

%\begin{center}
\begin{figure}[h]% <--- '[h]' says "let figure be here"
\begin{minipage}[c]{9.5cm}
\resizebox{1\textwidth}{!}{% for a smaller picture
\begin{tikzpicture}
 
% Input Layer
\foreach \i in {1,...,\inputnum}
{
    \node[circle, minimum size = 6mm, fill = orange!30] (Input-\i) at (0,-\i) {};
}
 
% Hidden Layer
\foreach \i in {1,...,\hiddennum}
{
    \node[circle, minimum size = 6mm, fill=teal!50, yshift = (\hiddennum-\inputnum)*5 mm] (Hidden-\i) at (2.5,-\i) {};
}

% Hidden Layer
\foreach \i in {1,...,\hiddennum}
{
    \node[circle, minimum size = 6mm, fill=teal!50, yshift = (\hiddennum-\inputnum)*5 mm] (Hiddenn-\i) at (5,-\i) {};
}

% Output Layer
\foreach \i in {1,...,\outputnum}
{
    \node[circle, minimum size = 6mm, fill = purple!50, yshift = (\outputnum-\inputnum)*5 mm] (Output-\i) at (7.5,-\i) {};
}
 
% Connect neurons In-Hidden
\foreach \i in {1,...,\inputnum}
{
    \foreach \j in {1,...,\hiddennum}
    {
        \draw[->, shorten >=1pt] (Input-\i) -- (Hidden-\j);   
    }
}
 
% Connect neurons In-hidden-In-hiddenn
\foreach \i in {1,...,\hiddennum}
{
    \foreach \j in {1,...,\hiddennum}
    {
        \draw[->, shorten >=1pt] (Hidden-\i) -- (Hiddenn-\j);   
    }
}

% Connect neurons Hidden-Out
\foreach \i in {1,...,\hiddennum}
{
    \foreach \j in {1,...,\outputnum}
    {
        \draw[->, shorten >=1pt] (Hiddenn-\i) -- (Output-\j);
    }
}
 
% Inputs
\foreach \i in {1,...,\inputnum}
{            
    \draw[<-, shorten <=1pt] (Input-\i) -- ++(-1,0) node[left]{$x_{noisy}$};
}
 
% Outputs
\foreach \i in {1,...,\outputnum}
{            
    \draw[->, shorten <=1pt] (Output-\i) -- ++(1,0) node[right]{$f(x_{noisy})$};
}


%\node[above] at (0,0 |- 0,2.5) {Input layer}; (0,0 |- 0,2.5)
%\node[above] at (0,0 |- 2.5,0) {Hidden layer 1};
%\node[above] at (Hiddenn-1 |- Output-1) {Hidden layer 2};
%\node[above] at (Output-1) {Output layer};
%\node[above] at (Hidden-1 |- Hiddenn-1) {Hidden layer 1};
%\node[above] at (Hiddenn-1 |- Output-1) {Hidden layer 2};
%\node[above] at (Output-1) {Output layer};
%\node[annot] at (I1 |- hl) {Input layer};
%\node[annot] at (O1 |- hl) {Output layer};
\end{tikzpicture}
%\end{center}
}% end reducing picture size
\end{minipage}%
\end{figure}

\vspace{7pt}

where $x_{noisy}$ is a noisy version of a clean patch $x$ and $f(x_{noisy})$ represents an estimate of $x$.

\end{frame}

\section{Setup}

\begin{frame}
% Expliqué INPUT/OUTPUT + POIDS W + LOSS (SNR)
\begin{block}{Weight initialization for MLP-based method}
Weights $w$ are sampled from an uniform distribution :  \\%Weight are randomly initialize following a uniform distribution:\\
$$w \sim \left[-\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}\right]$$ 
\end{block}



\begin{block}{Loss function}
The loss function used is the MSE : $$MSE = \frac{1}{n} \sum_{i=1}^{n} \ (f(x_i)-x_i),$$ where $f(x)$ the estimation $x$ and $x$ is a clean patch.
\end{block}


\begin{block}{Peak Signal-To-Noise Ratio (PSNR)}
PSNR = $20 \times \log_{10}\left(\dfrac{m}{\sqrt{\mathrm{MSE}}} \right)$ (dB), where $m$ is the maximum possible pixel value of a given image.
\end{block}
\end{frame}

\section{Results}

\begin{frame}{Results for AWG noise}
\begin{block}{Definition}
Additive white Gaussian noise (AWG) : Mimics the effect of many random processes that occur in nature. %(Source: Wiki)
% Présentez résultats images avec AWG noise (sigma=25)
\end{block} 
\end{frame}


\begin{frame}{Other type of noise}
% Présentez résultats images pour autres bruits (faire plusieurs slides)
\end{frame}

\section{Bounds}

\begin{frame}{Bounds}

% TROP BANCAL

\begin{block}{Clustering-based bounds}
There exist inherent limit on denoising quality for images with rich geometric structure.
\end{block}\

\begin{block}{Bayesian framework}
How well any denoising algorithm can perform, which depends on the patch size.
\end{block}

\end{frame}

\section{Block-matching}
\begin{frame}{Block-matching}
\begin{block}{Block-matching}
Idea: Find the patches most similar to a reference patch.
\end{block}\

\begin{block}{Combine MLP and block-matching}
Train MLPs that take as input a reference patch and its nearest neighbors (similar patches).
\end{block}\

\begin{block}{Results}
Block-matching MLPs provides better results on images with repeating structure than plain MLPs.

\vspace{0.5em}
However, BM3D and NLSC still provide better results on this kind of images.
\end{block}
\end{frame}






\section{Conclusion}

\begin{frame}{Conclusion}
\begin{block}{}

\end{block}
\end{frame}


\end{document}