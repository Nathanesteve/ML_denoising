\documentclass[10pt,a4paper]{article}
\usepackage[left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{longtable}
\usepackage{float}
\usepackage[justification=centering]{caption}
\usepackage{enumitem} % lists without bullets
%\usepackage{subcaption}
%\usepackage{graphicx}
%\usepackage{fancybox}
%\usepackage[dvipsnames,svgnames]{xcolor}
%\usepackage{nicematrix}
%\usepackage{tikz}
%\usetikzlibrary{fit}
%\usepackage{changepage}
\parindent 0ex
\usepackage[english]{babel}

\usepackage[dvipsnames]{xcolor}

% new commands
\newcommand{\svs}{\vspace{9pt}}
\newcommand{\mvs}{\vspace{27pt}}
\newcommand{\bvs}{\vspace{47pt}}
\newcommand{\ourparagraph}[1]{\paragraph{#1}}

\definecolor{newteal}{RGB}{0,118,155}
\usepackage{footnotebackref}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=Maroon,
    urlcolor=Maroon,
    citecolor=newteal}
%\usepackage{tcolorbox}

% references
\usepackage[backend=biber, citetracker=true, natbib=true]{biblatex}
%\usepackage[backend=biber,style=alphabetic,citetracker=true,natbib=true]{biblatex}
\usepackage[toc,page]{appendix}
\addbibresource{./references.bib}



\usepackage{notoccite}

% header
\author{Esteve Nathan, Fattouhy Mohamed, Nguyen Louis, Vernay Amélie}
\title{%
    \begin{minipage}\linewidth
        \centering
        Image denoising with multi-layer perceptrons
        \vskip3pt
        \large 
        HAX907X - Apprentissage statistique
        %\vskip3pt
        %Report
    \end{minipage}
}


\begin{document}

\maketitle
% Dans l'intro il faut parler du PSNR qui sera notre valeur pour determiner si on a fait un "bon denoising" ?
\section{Introduction}

The article we worked on, \emph{Image denoising with multi-layer perceptrons, part 1: comparison with existing algorithms and with bounds}, H. C. Burger, C. J. Schuler, S. Harmeling (2012) \citep{denoise}, aims to learn the mapping from a noisy image (image pixels undergo random fluctuations), to a noise-free image directly with plain multi-layer perceptrons (MLP).

\svs

Images are invariably corrupted by some degree of noise, which strength and type depends on the imaging process. Image denoising seeks to find a clean image given only its noisy version.

\svs

Its complexity requires to split the image into possibly overlapping smaller areas, called patches, denoised separately and then placed at the location of their noisy counterparts. However, the size of the patches affect the quality of the denoising function : large patches potentially lead to better result, but the function might be difficult to model.

\svs

Among the numerous existing types of noise, the authors mainly focus on additive white and Gaussian-distributed noise with known variance (AWG noise), but they show that the method can also be adapted to mixed Poisson-Gaussian noise, JPEG artifacts, salt and pepper noise and noise that resembles stripes.

\svs

%\section{Multi-layer perceptron}

% The calculations with this kind of matrices requires therefore great capacity of calculations. (transition...)

%The computationally most intensive operations in an MLP are %the matrix-vector multiplications. So for their experiments %they used Graphics Processing Units (GPUs) rather than Central %Processing Units (CPUs), because of their ability to %efficiently parallelize operations.

%MLP are very expensive in calculation time during their learning phase. Indeed the calibration of such a network requires a lot of algebraic calculation, more precisely matrix-vector products, which are the computationally most intensive operations.
%For the experiments, Graphics Processing Units (GPUs) are better than Central Processing Units (CPUs), because of their ability to efficiently parallelize operations.

%Expliquer les MLP et ajouter une phrase qui dit que dans la mesure où ce sont des matrix-verctor product c'est parallélisable et GPU...

% -----------------------------------------------------



\section{Multi-layer perceptron for image denoising}

A multi-layer perceptron is a particular architecture of neural network. In this architecture we have an input layer, an output layer and many hidden layers, each neuron of a hidden layer being connected to every neuron of the previous and the next ones. Signals between neurons are weighted by randomly initialized weights, updated by backpropagation to compute the gradient of a loss function.

\svs

To find a denoising function with MLP, the authors use pairs of noisy image patch (input) and clean image patch (output). To make it efficient, the data is normalized and the weights $w$ sampled from an uniform distribution :  \\%Weight are randomly initialize following a uniform distribution:\\
$$w \sim \left[-\frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}, \frac{\sqrt{6}}{\sqrt{n_j + n_{j+1}}}\right]$$ 

\svs
where $n_j$ and $n_{j+1}$ are the number of neurons in the input and output sides of the layer, respectively.

%Those steps ensure that all parts of the activation function, which is the sigmoid function, are reached.
\svs

To update these weights, they use the stochastic gradient descent, applied to our loss function, defined as the mean squared error between $f(x)$ (denoised patch) and $y$ (clean patch), minimizing pixel-wise. With this choise of loss function, we maximise the Peak Signal-To-Noise Ratio : PSNR = $20 \times \log_{10}(m/\sqrt{\mathrm{MSE}})$ (dB), where $m$ is the maximum possible pixel value of a given image. It is the ratio between the maximum possible power of a signal (interpretable signal) and the power of corrupting noise (stray signal)\footnote{\href{https://en.wikipedia.org/wiki/Peak\_signal-to-noise\_ratio}{https://en.wikipedia.org/wiki/Peak\_signal-to-noise\_ratio}}.

\svs

% Expliquer "stochastic gradient descent" (voir tp charlier)
Furthermore, to keep a steady learning rate while modifying the number $N$ of units per hidden layer, they divide it by $N$ in each layer. The basic learning rate was set to 0.1. 

\svs

The number of hidden layers, as well as $N$, determine the capacity of the model. In pratice, it's often better to use a large number of hidden layers with fewer hidden units each.

\svs

All experiments are performed on grey-image images, but the MLPs could also be trained on color images. They used images from six different datasets, and performed no pre-processing but the transform to grey-scale on the training images. 

\svs 

For each type of noise and each noise level, an MLP is trained. This allows us to obtain a
denoising function for each configuration.
Once the MLP's training is done, the authors compare the results obtained with other denoising methods.

%To evaluate their approche, they mainly focused on a standard test dataset, $standard\ test\ images$, and AWG noise with $\sigma=25$. However, they show results for others noise levels, other types of noise and other image sets, to compare the performance of different methods.


% Coût/GPU/BM3D

\svs
%After $3.5 \times 10^8$ backpropagations we obtain the denoising fonction that will be use to compare with the other method.

\section{Results and comparison with existing algorithms}

% Faire une sous-partie pour parler des 4 autres algo
% Faire une sous-partie pour AWG avec sigma qui varie.
% Faire une sous-partie avec d'autres bruits.

\ourparagraph{Existing algorithms -}{

Image denoising is a well-known problem, thus denoising methods are numerous and diverse. In order to evaluate the efficiency of the method, they compared the results against the following algorithms :

\svs

- \textbf{BM3D} does not explicitly use an image prior, but rather the fact that images often contain self-similarities. It's considered the state-of-the-art in image denoising ;

- \textbf{NLSC} is a dictionary-based algorithm that exploits images self-similarities like BM3D, also achieving excellent results ;

- \textbf{EPLL} is a learning-based approach, based on patch-based priors. It sometimes outperforms BM3D ;

- \textbf{KSVD} is a dictionary-based method, that approximates a noisy patch by a sparse linear combination of dictionary elements. It achieves better results than previous state-of-the-art algorithms.

\svs

The authors chose these algorithms for their comparison because they manage excellent results, with different approaches.}

\ourparagraph{Comparaison on AWG noise -}{
Let us present the results obtained with the MLP-based algorithm on AWG noise with $\sigma=25$.

\svs 

The MLP $(39 \times 2, 3072, 3072, 2559, 2047, 17 \times 2)$ was trained for approximately $3.5 \times 10^8$ backpropagations and delivered the best results.

\svs 

Out of the 11 \textit{standard test images}, the multi-layer perceptron approach achieves the best result on 7 images.
The method with MLP is clearly inferior to BM3D and NLSC on both of the images which contain a lot of regular structure. However, it outperforms KSVD on these images, even though KSVD is also an algorithm well-suited for images with regular structure. Furthermore, the MLP-based denoising algorithm also outperforms both KSVD and EPLL on every image of the dataset.

\svs

They now compare the MLP-based method to EPLL, BM3D and NLSC on the five larger test sets : \textit{Berkeley\ BSDS500}, \textit{Pascal VOC 2007}, \textit{Pascal VOC 2011}, \textit{McGill}, and \textit{ImageNet}, with a total of $2500$ test images.

\svs

The MLP-based method outperforms EPLL on $99.5\%$ of the $2500$ images, and BM3D on $92\%$ of it. It also outperforms NLSC on $80\%$ of the test sets ; the initial dictionary of NLSC was trained on a subset of \textit{Pascal VOC 2007}, which explains its good results.}

\ourparagraph{Comparison on different noise variances -}{
The results obtained by the approach with MLP on low noise ($\sigma = 10$), high noise ($\sigma = 50$),
very high noise ($\sigma = 75$) and extremely high noise ($\sigma = 170$) are presented below :

\svs

The MLP-based algorithm outperforms BM3D on $75.04\%$ of the $2500$ test images for $\sigma=10$, on $95.76\%$ of the test images for $\sigma=50$, and it outperforms BM3D on $97.60\%$ of the image set. For very high noise, the MLP-based method outperforms all other methods. The most performing methods for $\sigma=170$ are prior-based methods.

\svs 

To understand how the MLP-based method behaves on the noise levels they have been trained on, they varied $\sigma$ between $5$ and $100$ in steps of $5$.
Results show that the MLP-based approach achieves better results than BM3D on the noise levels it has been trained on.

\svs
However, for noise levels they have not been
trained on, the MLP-based algorithm performance degrades quickly.

Exceptions are the MLP-based methods trained on $\sigma = 50$ and $\sigma = 75$, which also outperform BM3D on $\sigma = 45$ and $\sigma = 55$ (MLP trained on $\sigma = 50$) and $\sigma = 70$ and $\sigma = 80$ (MLP trained on $\sigma = 75$).

\svs

To conclude, the MLP-based algorithm is particularly well-suited for high to very high noise
levels ($\sigma \geq 50$), and still outperforms BM3D with
$\sigma = 10$. However, MLPs have to be trained on each noise level in order to achieve good results.}

\section{Theoritical bound}

\ourparagraph{Clustering-based bounds -}{Recent denoising algorithms, especially BM3D, are very close to an inherent limit on denoising quality (estimated in \citep{deaddenoise}) for images with rich geometric structure. Yet, MLP outperforms BM3D by approximatively $0.4$ dB on this type of images, which is a significant improvement.


\ourparagraph{Bayesian bounds -}{Levin and Nadler \citep{Levin2011NaturalID} estimated theoretical bounds, in a Bayesian framework, on how well any denoising algorithm can perform, which depends on the patch size. The results with the MLP-based approach exceed these bounds, by using larger patches than assumed by Levin and Nadler.

Similar bounds for infinite patch sizes were estimated by Levin. The MLP-based algorithm makes significant progress in reaching these bounds: their approach achieves almost half the theoretically possible gain over BM3D.}

\section{Comparison on other types of noise}

Most denoising algorithms assume the noise to be AWG. It is actually not always the case, and for such algorithms, a transformation needs to be applied to obtain AWG noise. However, in most cases, such transformation is impossible to find. The method with MLP enables to learn a denoising algorithm for a given noise type, if it can be simulated. The authors use the architecture that achieved good results for AWG noise.

For stripe noise, the MLP-based approach outperforms BM3D on $82$ million training examples.

For salt and pepper noise, the method with the MLP-based algorithm outperforms both BM3D and median filtering, the latter being a common algorithm for denoising salt and pepper noise.

With regard to JPEG quantization artifacts, the MLP-based algorithm outperforms both the common method to enhance JPEG-compressed images and the state-of-the-art in JPEG deblocking.

In photon-limited imaging, observations are usually corrupted by mixed Poisson-Gaussian noise. On this type of noise, the method considered state-of-the-art is outperformed by the MLP-based approach.


\section{Block-matching MLPs}

Recent denoising algorithms such as BM3D and NLSC rely on block-matching procedures. The idea is to find the patches most similar to a reference patch.

Knowing that block-matching procedures are effective, let's see if a combination with the MLP-based algorithm can achieve better results, especially images with repeating structures, on which BM3D and NLSC outperform the approach with plain MLP.

The authors train MLPs that take as input a reference patch and its nearest neighbors (similar patches).

The block-matching procedure slows down the training by approximatively $2$. The MLP they used has four hidden layers $(4095, 2047, 2047, 2047)$. Compared to BM3D, this method always select the same number of neighbors, directly in the noisy image, and has fewer hyper-parameters.

\ourparagraph{Results -}{
The mean result achieved with block-matching MLP on the $11$ \textit{standard test images} is $0,07$ dB higher than the approach with plain MLP. It outperforms the plain MLP on $7$ images, especially on images with repeating structure. However, BM3D and NLSC still provide better results on this kind of images.

On larger test sets, both methods achieve quite the same results : method with plain MLPs performs better on images with smooth surfaces, while method with block-matching MLPs provides better results on images with repeating structure, whithout outperforming BM3D and NLSC.

However, it is noticeable that block-matching MLPs use less information as input than plain MLPs.}

\section{Conclusion}

The performances of their approach, which combining MLPs and patches, depends on the noise and strucure of images. On images with a lot of regular structure, MLPs achieves results that are much worse than the previous state-of-the-art. However, the MLPs perform very good results on images corrupting by high level of noise, especally on AWG noise, and this method was able to show that we could still get close to established theoretical barriers.
Finally, their attempt to use the MLP-based method with block-matching procedure did not provide significantly better results than the MLP.

\svs

The MLP-based approach manage excellent results, no matter the type of noise, especially with high levels of noise. It outperforms KSVD, NLSC, EPLL and BM3D on most of the test images. However, it achieves worst results than BM3D and NLSC on images with a lot of regular structures.

\svs

The results achieved by the MLP-based approach show that it is possible to significantly improve denoising quality on images with complex textures. Furthermore, it demonstrates that image priors are still useful at high noise levels. On bounds estimated for patches of infinite size, the method reaches half the theoretically possible gain over BM3D.

\svs

Once adapted to other types of noise, the MLP-based algorithm achives good results and even seems to be competitive with the state-of-the-art on JPEG quantization artifacts and mixed Poisson-Gaussian noise.

\svs

The block-matching procedure, at the cost of longer training and test time, enables to manage slightly better results, and deserves to be deepened in further researches.

\svs

Although the MLP-based method has a clear advantage on the other algorithms on some images, it sometimes performs much worse than BM3D and NLSC. Since the block-matching procedure has not proven its effectiveness, is it possible to find an approach that would perform state-of-the-art results on every image ? A combination of several denoising algorithms might be an answer.

\printbibliography

\mvs

\textbf{Link to our github repository}
\svs

\href{https://github.com/Nathanesteve/ML\_denoising}{https://github.com/Nathanesteve/ML\_denoising}


\end{document}