\documentclass[10pt,a4paper]{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{longtable}
\usepackage{float}
\usepackage[justification=centering]{caption}
\usepackage{enumitem} % lists without bullets
%\usepackage{subcaption}
%\usepackage{graphicx}
%\usepackage{fancybox}
%\usepackage[dvipsnames,svgnames]{xcolor}
%\usepackage{nicematrix}
%\usepackage{tikz}
%\usetikzlibrary{fit}
%\usepackage{changepage}
\parindent 0ex
\usepackage[english]{babel}

\usepackage[dvipsnames]{xcolor}

% new commands
\newcommand{\svs}{\vspace{9pt}}
\newcommand{\mvs}{\vspace{27pt}}
\newcommand{\bvs}{\vspace{47pt}}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}




\usepackage{footnotebackref}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=Maroon,
    urlcolor=Maroon}
%\usepackage{tcolorbox}

% references
\usepackage[backend=biber, citetracker=true, natbib=true]{biblatex}
%\usepackage[backend=biber,style=alphabetic,citetracker=true,natbib=true]{biblatex}
\usepackage[toc,page]{appendix}
\addbibresource{./references.bib}

% header
\author{Esteve Nathan, Fattouhy Mohamed, Nguyen Louis, Vernay Amélie}
\title{%
    \begin{minipage}\linewidth
        \centering
        Image denoising with multi-layer perceptrons
        \vskip3pt
        \large 
        HAX907X - Apprentissage statistique
        %\vskip3pt
        %Report
    \end{minipage}
}


\begin{document}

\maketitle

\section{Introduction}

The article we worked on \cite{denoise} aims to learn the mapping from a noisy image to a noise-free image directly with plain multi-layer perceptrons (MLP), applied to smaller areas, called patches. The denoised image is obtained by placing the denoised patches at the location of their noisy counterparts.

\svs

Images are invariably corrupted by some degree of noise, which strength and type depends on the imaging process. Image denoising seeks to find a clean image given only its noisy version.

\svs

Its complexity requires to split the image into possibly overlapping patches, denoised separately.

\svs

However, the size of the patches affect the quality of the denoising function : large patches potentially lead to better result, but the function might be difficult to model.

\svs

Among the numerous existing types of noise, we will mainly focus on additive white and Gaussian-distributed noise with known variance (AWG noise), but the method can also adapted to mixed Poisson-Gaussian noise, JPEG artifacts, salt and pepper noise and noise that resembles stripes.


\section{Multi-layer perceptron}

A multi-layer perceptron is a particular architecture of neural network. In this architecture we have many hidden layers, and each neuron of a hidden layer is connected to every neuron of the previous and the next ones.  In addition, weights are used to weight the signals between neurons. So with this kind of neural network, the weight matrix might be dense.

% The calculations with this kind of matrices requires therefore great capacity of calculations. (transiton...)

\svs 

The computationally most intensive operations in an MLP are the matrix-vector multiplications. So for their experiments they used Graphics Processing Units (GPUs) rather than Central Processing Units (CPUs), because of their ability to efficiently parallelize operations.


\svs



%Expliquer les MLP et ajouter une phrase qui dit que dans la mesure où ce sont des matrix-verctor product c'est parallélisable et GPU...

% -----------------------------------------------------



\section{MLP for image denoising}


To use an MLP for image denoising, they estimate the parameters by training on pairs of noisy and clean image patches using stochastic gradient descent. 


\svs

To make it efficient, they normalize the data and initialize the weights, which are sampled from an uniform distribution. Those two steps ensure that all parts of the sigmoid function are reached.

\svs

% Expliquer "stochastic gradient descent" (voir tp charlier)
Furthermore, to keep a steady learning rate while modifying the number $N$ of hidden units per layer, they divide it by $N$ in each layer.

\svs

The number of hidden layers as well as $N$, determine the capacity of the model. In pratice, it's often better to use a lot of hidden layers with few hidden units each.

\svs

\section{Experimental design}

All experiments are performed on grey-image images, but the MLPs could also be trained on color images. They used images from the imagenet dataset and they performed no pre-processing, but the transform to grey-scale on the training images, which belong to six different datasets.

\svs 

To evaluate their approche, they mainly focused on a standard test dataset, $standard\ test\ images$, and AWG noise with $\sigma=25$. However, they show results for others noise levels, other types of noise and other image sets, to compare the performances of methods.


% Coût/GPU/BM3D


\section{Results and comparison with existing algorithms}

% Faire une sous-partie pour parler des 4 autres algo
% Faire une sous-partie pour AWG avec sigma qui varie.
% Faire une sous-partie avec d'autres bruits.


\subsection{Existing algorithms}

There are many algorithms to denoise an image. In order to evaluate the efficiency of their method, they compared the results obtained against four algorithms, which we will briefly describe.

\svs

The algorithms are the following :

% Expliquer les algos.


They choose those algorithms because they achieve good results.


\subsection{Comparaison on AWG noise}

In this section, we present results achieved with an MLP on AWG noise with $\sigma=25$.

\svs 

The MLP had the architecture $(39 \times 2, 3072, 3072, 2559, 2047, 17 \times 2)$, and they chose that because it delivered the best results. Furthermore, the MLP was trained for approximately $3.5 \times 10^8$ backprops.

\svs 

On the 11 standard test images, their approach achieves the best result on 7 of the 11 test images and is the runner-up on one image. However, their method is clearly inferior to BM3D and NLSC on images which contain a lot of regular structure but outperform KSVD on those images, even though KSVD is also an algorithm that is well-suited for these types of images. In addition, they also outperform both KSVD and EPLL on every
image.

\svs

Now we compare their method to EPLL, BM3D and NLSC on the five larger test sets : $Berkeley\ BSDS500$, $Pascal\ VOC\ 2007$, $Pascal\ VOC\ 2011$, $McGill$, $ImageNet$. 

Each dataset contains 500 images, giving us a total of 2500 test images.

\subsection{Comparison on different noise variances}

Now we present the results obtained by their approach on other noise levels. They tested four noise levels : $\sigma = 10$ (low noise), $\sigma = 50$
(high noise), $\sigma = 75$ (very high noise) and $\sigma = 170$ (extremely high noise).


\printbibliography

\end{document}